{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104982e6-f8ad-4690-820c-cbb0e452de24",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c354e711-4370-4d39-9e98-9bb51335b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch_geometric pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fc01bf-0bf6-42eb-bb09-c8939c2de00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATv2Conv, SAGEConv, GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "RUN_PATH = './raw_graph'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52b029-8077-46b7-a9a6-f80afab713e7",
   "metadata": {},
   "source": [
    "## Parse Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a89e1b9-bf18-49be-809a-59b24a0600da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_formatted(filepath):\n",
    "    globals_ = {}\n",
    "    bounds = {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    f.close()\n",
    "    for l in lines:\n",
    "        if '=' in l:\n",
    "            key, val = l.split('=', 1)\n",
    "            key = key.strip()\n",
    "            if key in {'lx', 'ly', 'ux', 'uy'}:\n",
    "                bounds[key] = float(val.strip())\n",
    "            else:\n",
    "                globals_[key] = float(val.strip())\n",
    "    records = [ast.literal_eval(l) for l in lines if l.strip().startswith('{')]\n",
    "    drivers = [r['driver']['id'] for r in records]\n",
    "    sinks = [s['id'] for r in records for s in r['sinks']]\n",
    "    node_ids = sorted(set(drivers + sinks))\n",
    "    return node_ids, records, globals_, bounds\n",
    "\n",
    "def parse_label_formatted(label_path, node_ids, bounds):\n",
    "    lines = []\n",
    "    with open(label_path, 'r') as f:\n",
    "        for row in f:\n",
    "            parts = row.strip().split()\n",
    "            if len(parts) == 3 and parts[0].isdigit():\n",
    "                lines.append((int(parts[0]), float(parts[1]), float(parts[2])))\n",
    "    f.close()\n",
    "    ids, xs, ys = zip(*lines)\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    lx, ly, ux, uy = bounds['lx'], bounds['ly'], bounds['ux'], bounds['uy']\n",
    "\n",
    "    x_norm = (xs - lx) / (ux - lx)\n",
    "    y_norm = (ys - ly) / (uy - ly)\n",
    "    \n",
    "    # clamp if outside lower or upper bound  \n",
    "    x_norm = np.clip(x_norm, 0.0, 1.0)\n",
    "    y_norm = np.clip(y_norm, 0.0, 1.0)\n",
    "    # map back to node order\n",
    "    id2coord = {i: (x_norm[idx], y_norm[idx]) for idx, i in enumerate(ids)}\n",
    "    coords = [id2coord.get(nid, (0.0, 0.0)) for nid in node_ids]\n",
    "    return torch.tensor(coords, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ecdec-6c4c-4633-a33d-3a97b1259dd3",
   "metadata": {},
   "source": [
    "## Matrix/Feature Generation and Relative Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76a172ea-4940-4968-8e27-9885ea462884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_edge_index(node_ids, records, bidirectional=True):\n",
    "    id2idx = {nid: i for i, nid in enumerate(node_ids)}\n",
    "    edges = []\n",
    "    for r in records:\n",
    "        d = id2idx[r['driver']['id']]\n",
    "        for s in r['sinks']:\n",
    "            sid = id2idx[s['id']]\n",
    "            edges.append((d, sid))\n",
    "            if bidirectional:\n",
    "                edges.append((sid, d))\n",
    "    if not edges:\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "    return torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "def build_adjacency(N, edge_index):\n",
    "    src, dst = edge_index.cpu().numpy()\n",
    "    return csr_matrix((np.ones(len(src)), (src, dst)), shape=(N, N))\n",
    "\n",
    "def compute_laplacian_eigenvectors(adj, k=10, normalized=True):\n",
    "    N = adj.shape[0]\n",
    "    k_eff = min(k, max(N-1, 0))\n",
    "    deg = np.array(adj.sum(axis=1)).flatten()\n",
    "    if normalized:\n",
    "        inv_s = np.where(deg > 0, 1.0/np.sqrt(deg), 0.0)\n",
    "        D = csr_matrix((inv_s, (range(N), range(N))), shape=adj.shape)\n",
    "        L = csr_matrix(np.eye(N)) - D @ adj @ D\n",
    "    else:\n",
    "        D = csr_matrix((deg, (range(N), range(N))), shape=adj.shape)\n",
    "        L = D - adj\n",
    "    if k_eff < 1:\n",
    "        return np.zeros((N, 0), dtype=np.float32)\n",
    "    try:\n",
    "        _, vecs = eigsh(L, k=k_eff+1, which='SM')\n",
    "    except:\n",
    "        _, vecs = np.linalg.eigh(L.toarray())\n",
    "    return vecs[:, 1:k_eff+1]\n",
    "\n",
    "def shuffle_nodes(data: Data) -> Data:\n",
    "    num_nodes = data.num_nodes\n",
    "    perm = torch.randperm(num_nodes)\n",
    "    \n",
    "    data.x = data.x[perm]\n",
    "    data.y = data.y[perm]\n",
    "\n",
    "    inv_perm = torch.empty_like(perm)\n",
    "    inv_perm[perm] = torch.arange(num_nodes)\n",
    "    data.edge_index = inv_perm[data.edge_index]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8a4f362-9402-427a-a01a-07f53a4f0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relative_loss(out, data, criterion):\n",
    "    edge_index = data.edge_index\n",
    "    \n",
    "    src, tgt = edge_index\n",
    "    pred_src, pred_tgt = out[src], out[tgt]\n",
    "    true_src, true_tgt = data.y[src], data.y[tgt]\n",
    "    pred_dist = torch.norm(pred_src - pred_tgt, dim=1)\n",
    "    true_dist = torch.norm(true_src - true_tgt, dim=1)\n",
    "    \n",
    "    loss = criterion(pred_dist, true_dist)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def soft_density_loss(x, y, cell_area=0.0001, bin_size=0.05, density_threshold=0.4, sigma=0.01):\n",
    "    device = x.device\n",
    "    x = torch.clamp(x, 0.0, 1.0)\n",
    "    y = torch.clamp(y, 0.0, 1.0)\n",
    "    x_bins = torch.arange(bin_size / 2, 1.0, bin_size, device=device)\n",
    "    y_bins = torch.arange(bin_size / 2, 1.0, bin_size, device=device)\n",
    "    x_centers, y_centers = torch.meshgrid(x_bins, y_bins, indexing='ij')\n",
    "\n",
    "    Bx, By = x_centers.shape\n",
    "    num_bins = Bx * By\n",
    "\n",
    "    x_centers_flat = x_centers.flatten().unsqueeze(0)\n",
    "    y_centers_flat = y_centers.flatten().unsqueeze(0)\n",
    "    x_expand = x.unsqueeze(1)  # (N, 1)\n",
    "    y_expand = y.unsqueeze(1)  # (N, 1)\n",
    "\n",
    "    dx2 = (x_expand - x_centers_flat) ** 2\n",
    "    dy2 = (y_expand - y_centers_flat) ** 2\n",
    "    gauss = torch.exp(-(dx2 + dy2) / (2 * sigma**2))  # (N, B)\n",
    "\n",
    "    density_per_bin = torch.sum(gauss, dim=0) * cell_area  # (B,)\n",
    "    bin_area = bin_size * bin_size\n",
    "    density_norm = density_per_bin / bin_area  # (B,)\n",
    "\n",
    "    penalty = torch.clamp(density_norm - density_threshold, min=0.0)\n",
    "    loss = penalty.sum()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def loss_function(out, data, criterion):\n",
    "    pred_x, pred_y = out[:, 0], out[:, 1]\n",
    "    density = soft_density_loss(pred_x, pred_y, cell_area=0.0001, bin_size=0.05, density_threshold=0.6, sigma=0.01)\n",
    "    loss = 0.3 * criterion(out, data.y) + 0.7 * compute_relative_loss(out, data, criterion) + 0.0 * density\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a4943-e1f9-4152-9da0-d3b71bd49add",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "768d588b-d13e-433a-8efb-05881edfc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(root_dir, train_list, test_list, design_filter=[], batch_size=16, shuffle=True):\n",
    "    train_data_list = []\n",
    "    test_data_list = []\n",
    "    for dp, _, files in os.walk(root_dir):\n",
    "        for f in files:\n",
    "            if not f.endswith('_formatted.txt') or f.endswith('_label_formatted.txt'):\n",
    "                continue\n",
    "            curr_file = f.rsplit('_', 6)[0]\n",
    "            if design_filter and curr_file not in design_filter:\n",
    "                continue\n",
    "            fp = os.path.join(dp, f)\n",
    "            lf = fp.replace('_formatted.txt', '_label_formatted.txt')\n",
    "            if not os.path.exists(lf):\n",
    "                continue\n",
    "            node_ids, records, globals_, bounds = parse_formatted(fp)\n",
    "            orig_coords = {rec['driver']['id']:(rec['driver']['x'], rec['driver']['y']) for rec in records}\n",
    "            for rec in records:\n",
    "                for s in rec['sinks']:\n",
    "                    orig_coords[s['id']] = (s['x'], s['y'])\n",
    "            feats = torch.tensor(\n",
    "                compute_laplacian_eigenvectors(\n",
    "                    build_adjacency(len(node_ids), build_edge_index(node_ids, records)), 10\n",
    "                ), dtype=torch.float\n",
    "            )\n",
    "            labels = parse_label_formatted(lf, node_ids, bounds)\n",
    "            # need to globally normalize these values\n",
    "            u_vec = torch.tensor([\n",
    "                (globals_['Core Aspect Ratio'] - 0.5) / 0.4,\n",
    "                (globals_['Utilization'] - 40.0) / 28.0,\n",
    "                (globals_['Place Density'] - 0.2) / 0.3,\n",
    "                (globals_['core_width']/1000000),\n",
    "                (globals_['core_height']/1000000)\n",
    "            ], dtype=torch.float).unsqueeze(0)\n",
    "            edges = build_edge_index(node_ids, records)\n",
    "            data = Data(x=feats*100, edge_index=edges, u=u_vec, y=labels)\n",
    "            data = shuffle_nodes(data)\n",
    "            data.to(device)\n",
    "            data.design_name = f.replace('_formatted.txt','')\n",
    "            data.node_ids = node_ids\n",
    "            data.bounds = bounds\n",
    "            data.orig_coords = orig_coords\n",
    "            data.fixed_ids = [rec['driver']['id'] for rec in records if rec['driver'].get('is_fixed')]\n",
    "            data.fixed_ids += [s['id'] for rec in records for s in rec['sinks'] if s.get('is_fixed')]\n",
    "            #random.shuffle(data)\n",
    "            if curr_file in train_list:\n",
    "                train_data_list.append(data)\n",
    "            elif curr_file in test_list:\n",
    "                test_data_list.append(data)\n",
    "    if shuffle:\n",
    "        random.shuffle(train_data_list)\n",
    "        random.shuffle(test_data_list)\n",
    "    #return DataLoader(train_data_list, batch_size=batch_size, shuffle=shuffle, exclude_keys=['orig_coords','node_ids','bounds','fixed_ids', 'records']), DataLoader(test_data_list, batch_size=batch_size, shuffle=shuffle, exclude_keys=['orig_coords','node_ids','bounds','fixed_ids', 'records'])\n",
    "    return DataLoader(train_data_list, batch_size=batch_size, shuffle=shuffle, exclude_keys=['orig_coords','bounds','fixed_ids', 'records']), DataLoader(test_data_list, batch_size=batch_size, shuffle=shuffle, exclude_keys=['orig_coords','bounds','fixed_ids', 'records'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526b086-9b09-4f90-9663-e16f524dafdc",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2fce072-ad29-4cb9-a846-bb96d660329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlacementGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64, num_layers=3, global_channels=3, conv_type='sage'):\n",
    "        super().__init__()\n",
    "        ConvMap = {'gat': GATv2Conv, 'sage': SAGEConv, 'gcn': GCNConv}\n",
    "        Conv = ConvMap[conv_type]\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(\n",
    "                Conv(\n",
    "                    in_channels  if i == 0 else hidden_channels,\n",
    "                    hidden_channels\n",
    "                )\n",
    "            )\n",
    "        self.norm = torch.nn.LayerNorm(hidden_channels)\n",
    "        self.post_lin = torch.nn.Linear(hidden_channels + global_channels, hidden_channels)\n",
    "        self.out_lin = torch.nn.Linear(hidden_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, u, edge_attr=None):  \n",
    "        for conv in self.convs:\n",
    "            if isinstance(conv, GATv2Conv):\n",
    "                x = conv(x, edge_index, edge_attr)\n",
    "            else:\n",
    "                x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = self.norm(x)\n",
    "        u_exp = u[batch]\n",
    "        h = torch.cat([x, u_exp], dim=1)\n",
    "        h = F.relu(self.post_lin(h))\n",
    "        return self.out_lin(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6680b35-f4ad-4363-a9c0-654aec6b59fa",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72e2a5-0dec-4d76-b596-0e1c4576275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only first time when uploading data\n",
    "!unzip \"{RUN_PATH}.zip\" -d '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eae5a972-43fc-455e-9c76-2b922d54ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    adjust_list = [80, 150, 300]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5518c5a-7965-4e49-b08e-ecfd9a3a6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, model, criterion, optimizer, epoch):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for data in train_dataset:\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch, data.u)\n",
    "    \n",
    "        loss = loss_function(out, data, criterion)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "    if (epoch) % 10 == 0:\n",
    "        print(f'Epoch {epoch} Training loss: MSE: {train_loss/len(train_dataset.dataset):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3afd8bb8-93bc-4d22-b184-fe56549fd9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(test_dataset, model, criterion):\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataset.dataset:\n",
    "            data = data.to(device)\n",
    "\n",
    "            batch_vec = torch.zeros(data.x.size(0), dtype=torch.long, device=device)\n",
    "            out = model(data.x, data.edge_index, batch_vec, data.u)\n",
    "            \n",
    "            loss = loss_function(out, data, criterion)\n",
    "    \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d8dc612e-68d4-426e-9c83-4494db1cf2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wrapper(model, train_dataset, test_dataset):\n",
    "    \n",
    "    lr = 5e-3\n",
    "    weight_decay = 2e-4\n",
    "    epochs = 200\n",
    "    # gat (0.0029) - lr = 8e-3 250, 350, 450 * 0.5, epochs = 500\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss().to(device)\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        # Training\n",
    "        train(train_dataset, model, criterion, optimizer, epoch)\n",
    "        # Validation\n",
    "        test_loss = validate(test_dataset, model, criterion)\n",
    "        \n",
    "        if (epoch) % 10 == 0:\n",
    "            print(f'Validation loss: MSE: {test_loss/len(test_dataset.dataset):.4f}\\n')\n",
    "    \n",
    "    print(\"Saving Model\")\n",
    "    torch.save(model.state_dict(), 'gnn_all.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa1df68-dab9-4f4a-a6a1-0c39d1b93f6a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9be8529d-d6dd-4711-80f5-93472254bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_wrapper(model, train_dataset, test_dataset):\n",
    "\n",
    "    PATH = \"./gnn_all.pth\"\n",
    "    state_dict = torch.load(PATH)\n",
    "    model.load_state_dict(state_dict)\n",
    "    os.makedirs(\"./pred\", exist_ok=True)\n",
    "    \n",
    "    total = 0.0\n",
    "    \n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_dataset.dataset:\n",
    "            formatted_fp = os.path.join(f\"{RUN_PATH}\", data.design_name + '_formatted.txt')\n",
    "            _, records, _, _ = parse_formatted(formatted_fp)\n",
    "            data = data.to(device)\n",
    "            batch_vec = torch.zeros(data.x.size(0), dtype=torch.long, device=device)\n",
    "            out = model(data.x, data.edge_index, batch_vec, data.u)\n",
    "            \n",
    "            loss = loss_function(out, data, criterion)\n",
    "            total += loss.item()\n",
    "            out = torch.clamp(out, min=0.0, max=1.0)\n",
    "            \n",
    "            lx, ux = data.bounds['lx'], data.bounds['ux']\n",
    "            ly, uy = data.bounds['ly'], data.bounds['uy']\n",
    "            scale  = torch.tensor([ux - lx, uy - ly], device=out.device)\n",
    "            offset = torch.tensor([lx, ly], device=out.device)\n",
    "            preds  = (out * scale + offset).cpu().numpy()\n",
    "            id2name = {}\n",
    "            fixed_ids = []\n",
    "            for rec in records:\n",
    "                d = rec['driver']\n",
    "                id2name[d['id']] = d.get('name', str(d['id']))\n",
    "                for s in rec['sinks']:\n",
    "                    id2name[s['id']] = s.get('name', str(s['id']))\n",
    "                    \n",
    "            node_ids   = data.node_ids.tolist() if torch.is_tensor(data.node_ids) else data.node_ids\n",
    "            names = [id2name.get(nid, str(nid)) for nid in node_ids]\n",
    "            if hasattr(data, 'fixed_ids') and data.fixed_ids:\n",
    "                mask = ~np.isin(node_ids, data.fixed_ids)\n",
    "                names = [name for name, m in zip(names, mask) if m]\n",
    "                preds = preds[mask]\n",
    "    \n",
    "            fname = f\"./pred/{data.design_name}_predictions.txt\"\n",
    "            with open(fname, \"w\", newline=\"\") as f:\n",
    "                f.write(f\"InstanceName x_center y_center\\n\")\n",
    "                for name, (xv, yv) in zip(names, preds):\n",
    "                    f.write(f\"{name} {xv:.4f} {yv:.4f}\\n\")\n",
    "            print(f\"Saved {fname}\")\n",
    "            f.close()\n",
    "            \n",
    "    print(f'MSE: {total/len(test_dataset.dataset):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae7d52-3189-4c1a-9fca-5310ba966231",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7131f874-176e-49f8-a996-c9e1b33dedca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading\n",
      "Data Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "\n",
    "print(\"Starting data loading\")\n",
    "#train_set = [\"gcd_nangate45\", \"ibex_nangate45\", \"aes_nangate45\", \"gcd_asap7\", \"ibex_asap7\", \"aes_asap7\", \"ariane136_nangate45\"]\n",
    "train_set = [\"gcd_nangate45\", \"ibex_nangate45\", \"aes_nangate45\", \"ibex_asap7\", \"aes_asap7\", \"ariane136_nangate45\"]\n",
    "#train_set = [\"gcd_nangate45\"]\n",
    "#test_set = [\"jpeg_asap7\", \"jpeg_nangate45\", \"swerv_wrapper_nangate45\"]\n",
    "test_set = [\"gcd_asap7\"]\n",
    "    \n",
    "train_dataset, test_dataset = load_all_data(f\"{RUN_PATH}\", train_set, test_set, design_filter=[], batch_size=8) \n",
    "print(\"Data Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b9591ed-66bc-49ca-b412-12cf7e4b1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, 'train_data.pt')\n",
    "torch.save(test_dataset, 'test_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84e70b8c-7284-4695-97d7-b6b2f4650d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load('train_data.pt')\n",
    "test_dataset = torch.load('test_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c8df1cd8-96ff-47dd-8105-d52727c90cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training loss: MSE: 0.0381\n",
      "Validation loss: MSE: 0.0694\n",
      "\n",
      "Epoch 10 Training loss: MSE: 0.0234\n",
      "Validation loss: MSE: 0.0691\n",
      "\n",
      "Epoch 20 Training loss: MSE: 0.0217\n",
      "Validation loss: MSE: 0.0670\n",
      "\n",
      "Epoch 30 Training loss: MSE: 0.0201\n",
      "Validation loss: MSE: 0.0668\n",
      "\n",
      "Epoch 40 Training loss: MSE: 0.0192\n",
      "Validation loss: MSE: 0.0663\n",
      "\n",
      "Epoch 50 Training loss: MSE: 0.0183\n",
      "Validation loss: MSE: 0.0644\n",
      "\n",
      "Epoch 60 Training loss: MSE: 0.0176\n",
      "Validation loss: MSE: 0.0669\n",
      "\n",
      "Epoch 70 Training loss: MSE: 0.0172\n",
      "Validation loss: MSE: 0.0665\n",
      "\n",
      "Epoch 80 Training loss: MSE: 0.0162\n",
      "Validation loss: MSE: 0.0660\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m PlacementGNN(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgat\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m    \n",
      "Cell \u001b[0;32mIn[60], line 15\u001b[0m, in \u001b[0;36mtrain_wrapper\u001b[0;34m(model, train_dataset, test_dataset)\u001b[0m\n\u001b[1;32m     13\u001b[0m adjust_learning_rate(optimizer, epoch)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m     17\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m validate(test_dataset, model, criterion)\n",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataset, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[0;32m----> 5\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch, data\u001b[38;5;241m.\u001b[39mu)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch_geometric/data/data.py:362\u001b[0m, in \u001b[0;36mBaseData.to\u001b[0;34m(self, device, non_blocking, *args)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    358\u001b[0m        non_blocking: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    only the ones given in :obj:`*args`.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch_geometric/data/data.py:342\u001b[0m, in \u001b[0;36mBaseData.apply\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03mthe ones given in :obj:`*args`.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstores:\n\u001b[0;32m--> 342\u001b[0m     \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch_geometric/data/storage.py:201\u001b[0m, in \u001b[0;36mBaseStorage.apply\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03mthe ones given in :obj:`*args`.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m \u001b[43mrecursive_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch_geometric/data/storage.py:903\u001b[0m, in \u001b[0;36mrecursive_apply\u001b[0;34m(data, func)\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\u001b[38;5;241m*\u001b[39m(recursive_apply(d, func) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data))\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mrecursive_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: recursive_apply(data[key], func) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data}\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch_geometric/data/storage.py:903\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\u001b[38;5;241m*\u001b[39m(recursive_apply(d, func) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data))\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mrecursive_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: recursive_apply(data[key], func) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data}\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch_geometric/data/storage.py:903\u001b[0m, in \u001b[0;36mrecursive_apply\u001b[0;34m(data, func)\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\u001b[38;5;241m*\u001b[39m(recursive_apply(d, func) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data))\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mrecursive_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: recursive_apply(data[key], func) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data}\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch_geometric/data/storage.py:903\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\u001b[38;5;241m*\u001b[39m(recursive_apply(d, func) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data))\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [recursive_apply(d, func) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: recursive_apply(data[key], func) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model = PlacementGNN(10, 64, 2, 5, 'gat').to(device)\n",
    "train_wrapper(model, train_dataset, test_dataset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99d59c1e-162b-4b9e-9060-ba1acc7fd8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ./pred/gcd_asap7_gcd_run_5_5_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_4_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_5_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_5_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_4_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_3_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_3_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_4_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_4_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_5_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_3_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_2_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_1_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_1_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_4_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_2_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_4_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_5_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_3_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_4_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_2_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_3_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_4_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_2_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_4_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_3_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_1_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_5_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_1_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_1_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_5_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_2_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_3_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_5_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_1_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_3_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_5_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_5_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_4_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_3_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_4_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_2_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_4_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_5_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_4_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_2_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_1_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_4_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_1_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_1_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_2_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_1_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_5_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_5_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_5_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_1_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_2_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_3_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_1_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_3_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_2_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_5_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_4_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_4_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_1_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_5_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_1_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_5_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_2_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_3_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_3_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_2_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_3_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_3_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_3_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_1_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_2_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_3_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_5_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_2_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_2_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_2_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_5_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_4_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_5_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_3_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_2_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_3_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_4_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_1_2_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_3_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_4_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_1_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_5_2_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_2_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_2_1_1_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_4_2_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_1_4_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_3_4_3_predictions.txt\n",
      "Saved ./pred/gcd_asap7_gcd_run_1_1_1_predictions.txt\n",
      "MSE: 0.0620\n"
     ]
    }
   ],
   "source": [
    "infer_wrapper(model, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da34710-8f93-4ca7-a7c7-2cb1fa03812a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60a8ef-94f4-4ec7-8fb2-66afb26ea6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run: tar -czvf pred.tar.gz pred/\n",
    "# Transfer and untar: tar -xzvf pred.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ee3c47-fa7e-4c3b-97ba-c9b2f861dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add layer norm after gnnconv before MLP\n",
    "# try shuffling nodes in each graph\n",
    "# more data (synthetic data with different distributions)\n",
    "# squeeze output to fit within bounds (scale + offset)\n",
    "# tune the absolute supervised and relative supervsied parameters\n",
    "# try clustering\n",
    "# add penalties once in a good place\n",
    "# ask about bob place for clustering? - TA says hmetis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
