{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104982e6-f8ad-4690-820c-cbb0e452de24",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c354e711-4370-4d39-9e98-9bb51335b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch_geometric pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc01bf-0bf6-42eb-bb09-c8939c2de00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATv2Conv, SAGEConv, GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "RUN_PATH = './raw_graph'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52b029-8077-46b7-a9a6-f80afab713e7",
   "metadata": {},
   "source": [
    "## Parse Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a89e1b9-bf18-49be-809a-59b24a0600da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_formatted(filepath):\n",
    "    globals_ = {}\n",
    "    bounds = {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    f.close()\n",
    "    for l in lines:\n",
    "        if '=' in l:\n",
    "            key, val = l.split('=', 1)\n",
    "            key = key.strip()\n",
    "            if key in {'lx', 'ly', 'ux', 'uy'}:\n",
    "                bounds[key] = float(val.strip())\n",
    "            else:\n",
    "                globals_[key] = float(val.strip())\n",
    "    records = [ast.literal_eval(l) for l in lines if l.strip().startswith('{')]\n",
    "    drivers = [r['driver']['id'] for r in records]\n",
    "    sinks = [s['id'] for r in records for s in r['sinks']]\n",
    "    node_ids = sorted(set(drivers + sinks))\n",
    "    return node_ids, records, globals_, bounds\n",
    "\n",
    "def parse_label_formatted(label_path, node_ids, bounds):\n",
    "    lines = []\n",
    "    with open(label_path, 'r') as f:\n",
    "        for row in f:\n",
    "            parts = row.strip().split()\n",
    "            if len(parts) == 3 and parts[0].isdigit():\n",
    "                lines.append((int(parts[0]), float(parts[1]), float(parts[2])))\n",
    "    f.close()\n",
    "    ids, xs, ys = zip(*lines)\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    lx, ly, ux, uy = bounds['lx'], bounds['ly'], bounds['ux'], bounds['uy']\n",
    "\n",
    "    x_norm = (xs - lx) / (ux - lx)\n",
    "    y_norm = (ys - ly) / (uy - ly)\n",
    "    \n",
    "    # clamp if outside lower or upper bound  \n",
    "    x_norm = np.clip(x_norm, 0.0, 1.0)\n",
    "    y_norm = np.clip(y_norm, 0.0, 1.0)\n",
    "    # map back to node order\n",
    "    id2coord = {i: (x_norm[idx], y_norm[idx]) for idx, i in enumerate(ids)}\n",
    "    coords = [id2coord.get(nid, (0.0, 0.0)) for nid in node_ids]\n",
    "    return torch.tensor(coords, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ecdec-6c4c-4633-a33d-3a97b1259dd3",
   "metadata": {},
   "source": [
    "## Matrix/Feature Generation and Relative Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a172ea-4940-4968-8e27-9885ea462884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_edge_index(node_ids, records, bidirectional=True):\n",
    "    id2idx = {nid: i for i, nid in enumerate(node_ids)}\n",
    "    edges = []\n",
    "    for r in records:\n",
    "        d = id2idx[r['driver']['id']]\n",
    "        for s in r['sinks']:\n",
    "            sid = id2idx[s['id']]\n",
    "            edges.append((d, sid))\n",
    "            if bidirectional:\n",
    "                edges.append((sid, d))\n",
    "    if not edges:\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "    return torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "def build_adjacency(N, edge_index):\n",
    "    src, dst = edge_index.cpu().numpy()\n",
    "    return csr_matrix((np.ones(len(src)), (src, dst)), shape=(N, N))\n",
    "\n",
    "def compute_laplacian_eigenvectors(adj, k=10, normalized=True):\n",
    "    N = adj.shape[0]\n",
    "    k_eff = min(k, max(N-1, 0))\n",
    "    deg = np.array(adj.sum(axis=1)).flatten()\n",
    "    if normalized:\n",
    "        inv_s = np.where(deg > 0, 1.0/np.sqrt(deg), 0.0)\n",
    "        D = csr_matrix((inv_s, (range(N), range(N))), shape=adj.shape)\n",
    "        L = csr_matrix(np.eye(N)) - D @ adj @ D\n",
    "    else:\n",
    "        D = csr_matrix((deg, (range(N), range(N))), shape=adj.shape)\n",
    "        L = D - adj\n",
    "    if k_eff < 1:\n",
    "        return np.zeros((N, 0), dtype=np.float32)\n",
    "    try:\n",
    "        _, vecs = eigsh(L, k=k_eff+1, which='SM')\n",
    "    except:\n",
    "        _, vecs = np.linalg.eigh(L.toarray())\n",
    "    return vecs[:, 1:k_eff+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a4f362-9402-427a-a01a-07f53a4f0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relative_loss(out, data, criterion):\n",
    "    edge_index = data.edge_index\n",
    "    \n",
    "    src, tgt = edge_index\n",
    "    pred_src, pred_tgt = out[src], out[tgt]\n",
    "    true_src, true_tgt = data.y[src], data.y[tgt]\n",
    "    pred_dist = torch.norm(pred_src - pred_tgt, dim=1)\n",
    "    true_dist = torch.norm(true_src - true_tgt, dim=1)\n",
    "    \n",
    "    loss = criterion(pred_dist, true_dist)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def density_loss(x, y, cell_area=0.0001, bin_size=0.05, density_threshold=0.4, sigma=0.01):\n",
    "    device = x.device\n",
    "    x = torch.clamp(x, 0.0, 1.0)\n",
    "    y = torch.clamp(y, 0.0, 1.0)\n",
    "    x_bins = torch.arange(bin_size / 2, 1.0, bin_size, device=device)\n",
    "    y_bins = torch.arange(bin_size / 2, 1.0, bin_size, device=device)\n",
    "    x_centers, y_centers = torch.meshgrid(x_bins, y_bins, indexing='ij')\n",
    "\n",
    "    Bx, By = x_centers.shape\n",
    "    num_bins = Bx * By\n",
    "\n",
    "    x_centers_flat = x_centers.flatten().unsqueeze(0)\n",
    "    y_centers_flat = y_centers.flatten().unsqueeze(0)\n",
    "    x_expand = x.unsqueeze(1)  # (N, 1)\n",
    "    y_expand = y.unsqueeze(1)  # (N, 1)\n",
    "\n",
    "    dx2 = (x_expand - x_centers_flat) ** 2\n",
    "    dy2 = (y_expand - y_centers_flat) ** 2\n",
    "    gauss = torch.exp(-(dx2 + dy2) / (2 * sigma**2))  # (N, B)\n",
    "\n",
    "    density_per_bin = torch.sum(gauss, dim=0) * cell_area  # (B,)\n",
    "    bin_area = bin_size * bin_size\n",
    "    density_norm = density_per_bin / bin_area  # (B,)\n",
    "\n",
    "    penalty = torch.clamp(density_norm - density_threshold, min=0.0)\n",
    "    loss = penalty.sum()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a4943-e1f9-4152-9da0-d3b71bd49add",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d588b-d13e-433a-8efb-05881edfc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(root_dir, train_list, test_list, design_filter=[], batch_size=16, shuffle=True):\n",
    "    train_data_list = []\n",
    "    test_data_list = []\n",
    "    for dp, _, files in os.walk(root_dir):\n",
    "        for f in files:\n",
    "            if not f.endswith('_formatted.txt') or f.endswith('_label_formatted.txt'):\n",
    "                continue\n",
    "            curr_file = f.rsplit('_', 6)[0]\n",
    "            if design_filter and curr_file not in design_filter:\n",
    "                continue\n",
    "            fp = os.path.join(dp, f)\n",
    "            lf = fp.replace('_formatted.txt', '_label_formatted.txt')\n",
    "            if not os.path.exists(lf):\n",
    "                continue\n",
    "            node_ids, records, globals_, bounds = parse_formatted(fp)\n",
    "            orig_coords = {rec['driver']['id']:(rec['driver']['x'], rec['driver']['y']) for rec in records}\n",
    "            for rec in records:\n",
    "                for s in rec['sinks']:\n",
    "                    orig_coords[s['id']] = (s['x'], s['y'])\n",
    "            feats = torch.tensor(\n",
    "                compute_laplacian_eigenvectors(\n",
    "                    build_adjacency(len(node_ids), build_edge_index(node_ids, records)), 10\n",
    "                ), dtype=torch.float\n",
    "            )\n",
    "            labels = parse_label_formatted(lf, node_ids, bounds)\n",
    "            # need to globally normalize these values\n",
    "            u_vec = torch.tensor([\n",
    "                (globals_['Core Aspect Ratio'] - 0.5) / 0.4,\n",
    "                (globals_['Utilization'] - 40.0) / 28.0,\n",
    "                (globals_['Place Density'] - 0.2) / 0.3,\n",
    "                (globals_['core_width']/1000000),\n",
    "                (globals_['core_height']/1000000)\n",
    "            ], dtype=torch.float).unsqueeze(0)\n",
    "            edges = build_edge_index(node_ids, records)\n",
    "            data = Data(x=feats*100, edge_index=edges, u=u_vec, y=labels)\n",
    "            data.to(device)\n",
    "            data.design_name = f.replace('_formatted.txt','')\n",
    "            data.node_ids = node_ids\n",
    "            data.bounds = bounds\n",
    "            data.orig_coords = orig_coords\n",
    "            data.fixed_ids = [rec['driver']['id'] for rec in records if rec['driver'].get('is_fixed')]\n",
    "            data.fixed_ids += [s['id'] for rec in records for s in rec['sinks'] if s.get('is_fixed')]\n",
    "            if curr_file in train_list:\n",
    "                train_data_list.append(data)\n",
    "            elif curr_file in test_list:\n",
    "                test_data_list.append(data)\n",
    "    if shuffle:\n",
    "        random.shuffle(train_data_list)\n",
    "        random.shuffle(test_data_list)\n",
    "    return DataLoader(train_data_list, batch_size=batch_size, shuffle=shuffle, exclude_keys=['orig_coords','node_ids','bounds','fixed_ids', 'records']), DataLoader(test_data_list, batch_size=batch_size, shuffle=shuffle, exclude_keys=['orig_coords','node_ids','bounds','fixed_ids', 'records'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526b086-9b09-4f90-9663-e16f524dafdc",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fce072-ad29-4cb9-a846-bb96d660329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlacementGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64, num_layers=3, global_channels=3, conv_type='sage'):\n",
    "        super().__init__()\n",
    "        ConvMap = {'gat': GATv2Conv, 'sage': SAGEConv, 'gcn': GCNConv}\n",
    "        Conv = ConvMap[conv_type]\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(\n",
    "                Conv(\n",
    "                    in_channels  if i == 0 else hidden_channels,\n",
    "                    hidden_channels\n",
    "                )\n",
    "            )\n",
    "        self.post_lin = torch.nn.Linear(hidden_channels + global_channels, hidden_channels)\n",
    "        self.out_lin = torch.nn.Linear(hidden_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, u, edge_attr=None):  \n",
    "        for conv in self.convs:\n",
    "            if isinstance(conv, GATv2Conv):\n",
    "                x = conv(x, edge_index, edge_attr)\n",
    "            else:\n",
    "                x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        u_exp = u[batch]\n",
    "        h = torch.cat([x, u_exp], dim=1)\n",
    "        h = F.relu(self.post_lin(h))\n",
    "        return self.out_lin(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6680b35-f4ad-4363-a9c0-654aec6b59fa",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72e2a5-0dec-4d76-b596-0e1c4576275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip \"{RUN_PATH}.zip\" -d '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae5a972-43fc-455e-9c76-2b922d54ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    adjust_list = [80, 150, 300]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5518c5a-7965-4e49-b08e-ecfd9a3a6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, model, criterion, optimizer, epoch):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for data in train_dataset:\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch, data.u)\n",
    "        pred_x, pred_y = out[:, 0], out[:, 1]\n",
    "        density = soft_density_loss(pred_x, pred_y, cell_area=0.0001, bin_size=0.05, density_threshold=0.6, sigma=0.01)\n",
    "        loss = 0.0 * criterion(out, data.y) + 1.0 * compute_relative_loss(out, data, criterion) + 0.0 * density\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "    if (epoch) % 10 == 0:\n",
    "        print(f'Epoch {epoch} Training loss: MSE: {train_loss/len(train_dataset.dataset):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd8bb8-93bc-4d22-b184-fe56549fd9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(test_dataset, model, criterion):\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataset.dataset:\n",
    "            data = data.to(device)\n",
    "\n",
    "            batch_vec = torch.zeros(data.x.size(0), dtype=torch.long, device=device)\n",
    "            out = model(data.x, data.edge_index, batch_vec, data.u)\n",
    "            pred_x, pred_y = out[:, 0], out[:, 1]\n",
    "            density = soft_density_loss(pred_x, pred_y, cell_area=0.0001, bin_size=0.05, density_threshold=0.6, sigma=0.01)\n",
    "            loss = 0.0 * criterion(out, data.y) + 1.0 * compute_relative_loss(out, data, criterion) + 0.0 * density\n",
    "    \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc612e-68d4-426e-9c83-4494db1cf2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wrapper(train_dataset, test_dataset):\n",
    "    \n",
    "    lr = 5e-3\n",
    "    weight_decay = 1e-4\n",
    "    epochs = 500\n",
    "    # gat (0.0029) - lr = 8e-3 250, 350, 450 * 0.5, epochs = 500\n",
    "    \n",
    "    model = PlacementGNN(10, 64, 3, 5, 'sage').to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss().to(device)\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        # Training\n",
    "        train(train_dataset, model, criterion, optimizer, epoch)\n",
    "        # Validation\n",
    "        test_loss = validate(test_dataset, model, criterion)\n",
    "        \n",
    "        if (epoch) % 10 == 0:\n",
    "            print(f'Validation loss: MSE: {test_loss/len(test_dataset.dataset):.4f}\\n')\n",
    "    \n",
    "    print(\"Saving Model\")\n",
    "    torch.save(model.state_dict(), 'gnn_all.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa1df68-dab9-4f4a-a6a1-0c39d1b93f6a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be8529d-d6dd-4711-80f5-93472254bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_wrapper(train_dataset, test_dataset):\n",
    "    model = PlacementGNN(10, 64, 4, 5, 'gat').to(device)\n",
    "    PATH = \"./gnn_all.pth\"\n",
    "    state_dict = torch.load(PATH)\n",
    "    model.load_state_dict(state_dict)\n",
    "    os.makedirs(\"./pred\", exist_ok=True)\n",
    "    \n",
    "    total = 0.0\n",
    "    \n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_dataset.dataset:\n",
    "            formatted_fp = os.path.join(f\"{RUN_PATH}\", data.design_name + '_formatted.txt')\n",
    "            _, records, _, _ = parse_formatted(formatted_fp)\n",
    "            data = data.to(device)\n",
    "            batch_vec = torch.zeros(data.x.size(0), dtype=torch.long, device=device)\n",
    "            out = model(data.x, data.edge_index, batch_vec, data.u)\n",
    "            pred_x, pred_y = out[:, 0], out[:, 1]\n",
    "            density = soft_density_loss(pred_x, pred_y, cell_area=0.0001, bin_size=0.05, density_threshold=0.6, sigma=0.01)\n",
    "            loss = 0.0 * criterion(out, data.y) + 1.0 * compute_relative_loss(out, data, criterion) + 0.0 * density\n",
    "            total += loss.item()\n",
    "            lx, ux = data.bounds['lx'], data.bounds['ux']\n",
    "            ly, uy = data.bounds['ly'], data.bounds['uy']\n",
    "            scale  = torch.tensor([ux - lx, uy - ly], device=out.device)\n",
    "            offset = torch.tensor([lx, ly], device=out.device)\n",
    "            preds  = (out * scale + offset).cpu().numpy()\n",
    "            id2name = {}\n",
    "            fixed_ids = []\n",
    "            for rec in records:\n",
    "                d = rec['driver']\n",
    "                id2name[d['id']] = d.get('name', str(d['id']))\n",
    "                for s in rec['sinks']:\n",
    "                    id2name[s['id']] = s.get('name', str(s['id']))\n",
    "                    \n",
    "            node_ids   = data.node_ids.tolist() if torch.is_tensor(data.node_ids) else data.node_ids\n",
    "            names = [id2name.get(nid, str(nid)) for nid in node_ids]\n",
    "            if hasattr(data, 'fixed_ids') and data.fixed_ids:\n",
    "                mask = ~np.isin(node_ids, data.fixed_ids)\n",
    "                names = [name for name, m in zip(names, mask) if m]\n",
    "                preds = preds[mask]\n",
    "    \n",
    "            fname = f\"./pred/{data.design_name}_predictions.txt\"\n",
    "            with open(fname, \"w\", newline=\"\") as f:\n",
    "                f.write(f\"InstanceName x_center y_center\\n\")\n",
    "                for name, (xv, yv) in zip(names, preds):\n",
    "                    f.write(f\"{name} {xv:.4f} {yv:.4f}\\n\")\n",
    "            print(f\"Saved {fname}\")\n",
    "            f.close()\n",
    "            \n",
    "    print(f'MSE: {total/len(test_dataset.dataset):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7131f874-176e-49f8-a996-c9e1b33dedca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting data loading\")\n",
    "\n",
    "#train_set = [\"gcd_nangate45\", \"ibex_nangate45\", \"aes_nangate45\", \"gcd_asap7\", \"ibex_asap7\", \"aes_asap7\", \"ariane136_nangate45\"]\n",
    "train_set = [\"gcd_nangate45\"]\n",
    "#test_set = [\"jpeg_asap7\", \"jpeg_nangate45\", \"swerv_wrapper_nangate45\"]\n",
    "test_set = [\"gcd_asap7\"]\n",
    "    \n",
    "train_dataset, test_dataset = load_all_data(f\"{RUN_PATH}\", train_set, test_set, design_filter=[\"gcd_nangate45\", \"gcd_asap7\"], batch_size=8) \n",
    "    \n",
    "print(\"Data Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df1cd8-96ff-47dd-8105-d52727c90cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wrapper(train_dataset, test_dataset)    \n",
    "#infer_wrapper(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60a8ef-94f4-4ec7-8fb2-66afb26ea6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run: tar -czvf pred.tar.gz pred/\n",
    "# Transfer and untar: tar -xzvf pred.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666522a3-363e-4b54-ab0d-16b95ee6029f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
