{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104982e6-f8ad-4690-820c-cbb0e452de24",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fc01bf-0bf6-42eb-bb09-c8939c2de00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from torch_geometric.data import Data, DataLoader, Batch\n",
    "from torch_geometric.nn import GATv2Conv, SAGEConv, GCNConv\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52b029-8077-46b7-a9a6-f80afab713e7",
   "metadata": {},
   "source": [
    "## Parse Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a89e1b9-bf18-49be-809a-59b24a0600da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_formatted(filepath):\n",
    "    globals_ = {}\n",
    "    bounds = {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    for l in lines:\n",
    "        if 'Core Aspect Ratio' in l:\n",
    "            globals_['Core Aspect Ratio'] = float(l.split('=')[-1].strip())\n",
    "        if 'Utilization' in l and 'Place Density' not in l:\n",
    "            globals_['Utilization'] = float(l.split('=')[-1].strip())\n",
    "        if 'Place Density' in l:\n",
    "            globals_['Place Density'] = float(l.split('=')[-1].strip())\n",
    "        if '=' in l:\n",
    "            key, val = l.split('=', 1)\n",
    "            key = key.strip()\n",
    "            if key in {'lx', 'ly', 'ux', 'uy'}:\n",
    "                bounds[key] = float(val.strip())\n",
    "    records = [ast.literal_eval(l) for l in lines if l.strip().startswith('{')]\n",
    "    drivers = [r['driver']['id'] for r in records]\n",
    "    sinks = [s['id'] for r in records for s in r['sinks']]\n",
    "    node_ids = sorted(set(drivers + sinks))\n",
    "    return node_ids, records, globals_, bounds\n",
    "\n",
    "def parse_label_formatted(label_path, node_ids, bounds):\n",
    "    lines = []\n",
    "    with open(label_path, 'r') as f:\n",
    "        for row in f:\n",
    "            parts = row.strip().split()\n",
    "            if len(parts) == 3 and parts[0].isdigit():\n",
    "                lines.append((int(parts[0]), float(parts[1]), float(parts[2])))\n",
    "    ids, xs, ys = zip(*lines)\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    lx, ly, ux, uy = bounds['lx'], bounds['ly'], bounds['ux'], bounds['uy']\n",
    "\n",
    "    x_norm = (xs - lx) / (ux - lx) \n",
    "    y_norm = (ys - ly) / (uy - ly) \n",
    "    \n",
    "    # clamp if outside lower or upper bound  \n",
    "    x_norm = np.clip(x_norm, 0.0, 1.0)\n",
    "    y_norm = np.clip(y_norm, 0.0, 1.0)\n",
    "    # map back to node order\n",
    "    id2coord = {i: (x_norm[idx], y_norm[idx]) for idx, i in enumerate(ids)}\n",
    "    coords = [id2coord.get(nid, (0.0, 0.0)) for nid in node_ids]\n",
    "    return torch.tensor(coords, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ecdec-6c4c-4633-a33d-3a97b1259dd3",
   "metadata": {},
   "source": [
    "## Matrix/Feature Generation and Relative Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a172ea-4940-4968-8e27-9885ea462884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_edge_index(node_ids, records, bidirectional=True):\n",
    "    id2idx = {nid: i for i, nid in enumerate(node_ids)}\n",
    "    edges = []\n",
    "    for r in records:\n",
    "        d = id2idx[r['driver']['id']]\n",
    "        for s in r['sinks']:\n",
    "            sid = id2idx[s['id']]\n",
    "            edges.append((d, sid))\n",
    "            if bidirectional:\n",
    "                edges.append((sid, d))\n",
    "    if not edges:\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "    return torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "def build_adjacency(N, edge_index):\n",
    "    src, dst = edge_index.cpu().numpy()\n",
    "    return csr_matrix((np.ones(len(src)), (src, dst)), shape=(N, N))\n",
    "\n",
    "def compute_laplacian_eigenvectors(adj, k=10, normalized=True):\n",
    "    N = adj.shape[0]\n",
    "    k_eff = min(k, max(N-1, 0))\n",
    "    deg = np.array(adj.sum(axis=1)).flatten()\n",
    "    if normalized:\n",
    "        inv_s = np.where(deg > 0, 1.0/np.sqrt(deg), 0.0)\n",
    "        D = csr_matrix((inv_s, (range(N), range(N))), shape=adj.shape)\n",
    "        L = csr_matrix(np.eye(N)) - D @ adj @ D\n",
    "    else:\n",
    "        D = csr_matrix((deg, (range(N), range(N))), shape=adj.shape)\n",
    "        L = D - adj\n",
    "    if k_eff < 1:\n",
    "        return np.zeros((N, 0), dtype=np.float32)\n",
    "    try:\n",
    "        _, vecs = eigsh(L, k=k_eff+1, which='SM')\n",
    "    except:\n",
    "        _, vecs = np.linalg.eigh(L.toarray())\n",
    "    return vecs[:, 1:k_eff+1]\n",
    "\n",
    "def compute_relative_loss(out, data):\n",
    "    id2idx = {nid: i for i, nid in enumerate(data.node_ids)}\n",
    "    rel_loss, cnt = 0.0, 0\n",
    "    for rec in data.records:\n",
    "        d_i = id2idx[rec['driver']['id']]\n",
    "        for sink in rec['sinks']:\n",
    "            s_i = id2idx[sink['id']]\n",
    "            pred_d, pred_s = out[d_i], out[s_i]\n",
    "            true_d, true_s = data.y[d_i], data.y[s_i]\n",
    "            rel_loss += (torch.norm(pred_d - pred_s) - torch.norm(true_d - true_s)).pow(2)\n",
    "            cnt += 1\n",
    "    return rel_loss / max(cnt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a4943-e1f9-4152-9da0-d3b71bd49add",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "768d588b-d13e-433a-8efb-05881edfc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(root_dir, design_filter='gcd', batch_size=16, shuffle=True):\n",
    "    data_list = []\n",
    "    for dp, _, files in os.walk(root_dir):\n",
    "        for f in files:\n",
    "            if not f.endswith('_formatted.txt') or f.endswith('_label_formatted.txt'):\n",
    "                continue\n",
    "            if design_filter and f.split('_')[0] != design_filter:\n",
    "                continue\n",
    "            fp = os.path.join(dp, f)\n",
    "            lf = fp.replace('_formatted.txt', '_label_formatted.txt')\n",
    "            if not os.path.exists(lf):\n",
    "                continue\n",
    "            node_ids, records, globals_, bounds = parse_formatted(fp)\n",
    "            orig_coords = {rec['driver']['id']:(rec['driver']['x'], rec['driver']['y']) for rec in records}\n",
    "            for rec in records:\n",
    "                for s in rec['sinks']:\n",
    "                    orig_coords[s['id']] = (s['x'], s['y'])\n",
    "            feats = torch.tensor(\n",
    "                compute_laplacian_eigenvectors(\n",
    "                    build_adjacency(len(node_ids), build_edge_index(node_ids, records)), 10\n",
    "                ), dtype=torch.float\n",
    "            )\n",
    "            labels = parse_label_formatted(lf, node_ids, bounds)\n",
    "            u_vec = torch.tensor([\n",
    "                (globals_['Core Aspect Ratio'] - 0.5) / 0.4,\n",
    "                (globals_['Utilization'] - 40.0) / 28.0,\n",
    "                (globals_['Place Density'] - 0.2) / 0.3\n",
    "            ], dtype=torch.float).unsqueeze(0)\n",
    "            edges = build_edge_index(node_ids, records)\n",
    "            data = Data(x=feats, edge_index=edges, u=u_vec, y=labels)\n",
    "            data.design_name = f.replace('_formatted.txt','')\n",
    "            data.node_ids = node_ids\n",
    "            data.bounds = bounds\n",
    "            data.orig_coords = orig_coords\n",
    "            data.fixed_ids = [rec['driver']['id'] for rec in records if rec['driver'].get('is_fixed')]\n",
    "            data.fixed_ids += [s['id'] for rec in records for s in rec['sinks'] if s.get('is_fixed')]\n",
    "            data_list.append(data)\n",
    "    if shuffle:\n",
    "        random.shuffle(data_list)\n",
    "    return DataLoader(data_list, batch_size=batch_size, shuffle=shuffle, exclude_keys=['orig_coords','node_ids','bounds','fixed_ids'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526b086-9b09-4f90-9663-e16f524dafdc",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2fce072-ad29-4cb9-a846-bb96d660329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlacementGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64, num_layers=3, global_channels=3, conv_type='sage'):\n",
    "        super().__init__()\n",
    "        ConvMap = {'gat': GATv2Conv, 'sage': SAGEConv, 'gcn': GCNConv}\n",
    "        Conv = ConvMap[conv_type]\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(\n",
    "                Conv(\n",
    "                    in_channels if i == 0 else hidden_channels,\n",
    "                    hidden_channels\n",
    "                )\n",
    "            )\n",
    "        self.post_lin = torch.nn.Linear(hidden_channels + global_channels, hidden_channels)\n",
    "        self.out_lin = torch.nn.Linear(hidden_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, u, edge_attr=None):\n",
    "        for conv in self.convs:\n",
    "            if isinstance(conv, GATv2Conv):\n",
    "                x = conv(x, edge_index, edge_attr)\n",
    "            else:\n",
    "                x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        u_exp = u[batch]\n",
    "        h = torch.cat([x, u_exp], dim=1)\n",
    "        h = F.relu(self.post_lin(h))\n",
    "        return self.out_lin(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6680b35-f4ad-4363-a9c0-654aec6b59fa",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e94bbed2-5de9-434e-b0e2-861106c413e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c3shah/.local/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded!\n",
      "Training\n",
      "x: torch.Size([4080, 10])\n",
      "x: torch.Size([3704, 10])\n",
      "x: torch.Size([3892, 10])\n",
      "x: torch.Size([3704, 10])\n",
      "x: torch.Size([3704, 10])\n",
      "x: torch.Size([4268, 10])\n",
      "x: torch.Size([3516, 10])\n",
      "x: torch.Size([3516, 10])\n",
      "x: torch.Size([3516, 10])\n",
      "x: torch.Size([3516, 10])\n",
      "x: torch.Size([3516, 10])\n",
      "x: torch.Size([2952, 10])\n",
      "x: torch.Size([3516, 10])\n",
      "x: torch.Size([3704, 10])\n",
      "x: torch.Size([4080, 10])\n",
      "x: torch.Size([3704, 10])\n",
      "x: torch.Size([3328, 10])\n",
      "x: torch.Size([3892, 10])\n",
      "x: torch.Size([3516, 10])\n",
      "x: torch.Size([4080, 10])\n",
      "x: torch.Size([3704, 10])\n",
      "x: torch.Size([3516, 10])\n",
      "x: torch.Size([3892, 10])\n",
      "x: torch.Size([4080, 10])\n",
      "x: torch.Size([3704, 10])\n",
      "Epoch 1 MSE: 0.1943\n",
      "Saving Model\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting\")\n",
    "loader = load_all_data('./raw_graph', design_filter='gcd', batch_size=8)\n",
    "print(\"Data Loaded!\")\n",
    "model = PlacementGNN(10, 64, 4, 3, 'sage').to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "print(\"Training\")\n",
    "for epoch in range(1, 2):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch, data.u)\n",
    "        print(\"x:\",data.x.shape)\n",
    "        loss = loss_fn(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += loss.item() * data.num_graphs\n",
    "    print(f'Epoch {epoch} MSE: {total/len(loader.dataset):.4f}')\n",
    "print(\"Saving Model\")\n",
    "torch.save(model.state_dict(), 'gnn_all.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa1df68-dab9-4f4a-a6a1-0c39d1b93f6a",
   "metadata": {},
   "source": [
    "## Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9be8529d-d6dd-4711-80f5-93472254bcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved gcd_nangate45_gcd_run_5_2_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_3_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_1_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_2_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_2_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_2_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_4_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_1_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_2_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_5_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_5_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_1_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_5_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_1_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_5_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_1_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_5_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_2_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_2_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_1_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_3_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_5_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_3_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_1_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_4_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_3_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_2_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_3_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_5_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_5_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_4_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_4_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_4_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_4_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_2_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_5_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_2_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_1_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_1_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_3_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_5_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_3_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_4_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_3_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_3_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_1_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_4_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_2_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_1_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_5_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_2_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_5_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_2_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_5_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_3_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_4_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_3_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_5_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_5_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_1_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_5_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_2_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_1_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_1_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_4_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_1_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_2_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_4_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_3_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_2_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_5_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_1_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_4_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_2_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_4_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_1_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_3_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_3_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_1_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_5_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_3_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_1_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_4_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_4_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_4_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_3_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_5_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_3_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_5_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_3_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_5_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_5_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_3_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_1_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_2_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_4_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_4_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_1_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_1_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_5_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_3_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_5_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_3_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_1_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_1_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_3_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_5_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_2_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_1_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_1_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_4_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_1_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_2_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_4_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_4_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_5_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_4_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_4_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_3_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_3_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_4_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_3_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_5_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_2_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_2_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_2_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_3_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_4_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_2_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_3_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_5_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_4_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_2_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_3_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_1_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_3_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_4_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_3_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_4_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_2_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_2_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_2_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_2_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_3_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_2_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_2_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_1_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_4_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_5_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_3_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_1_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_2_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_5_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_3_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_5_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_3_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_3_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_2_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_4_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_1_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_2_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_4_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_1_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_2_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_4_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_4_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_1_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_1_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_5_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_5_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_4_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_4_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_1_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_3_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_5_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_2_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_3_4_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_2_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_5_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_5_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_4_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_3_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_5_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_4_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_4_4_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_3_2_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_5_1_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_1_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_3_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_3_1_3_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_5_2_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_1_4_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_4_2_3_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_2_4_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_1_4_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_1_2_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_5_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_2_1_1_predictions.csv\n",
      "Saved gcd_asap7_gcd_run_5_3_1_predictions.csv\n",
      "Saved gcd_nangate45_gcd_run_2_5_4_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in loader.dataset:\n",
    "        formatted_fp = os.path.join('./raw_graph', data.design_name + '_formatted.txt')\n",
    "        _, records, _, _ = parse_formatted(formatted_fp)\n",
    "        data = data.to(device)\n",
    "        batch_vec = torch.zeros(data.x.size(0), dtype=torch.long, device=device)\n",
    "        out = model(data.x, data.edge_index, batch_vec, data.u)\n",
    "        lx, ux = data.bounds['lx'], data.bounds['ux']\n",
    "        ly, uy = data.bounds['ly'], data.bounds['uy']\n",
    "        scale  = torch.tensor([ux - lx, uy - ly], device=out.device)\n",
    "        offset = torch.tensor([lx, ly], device=out.device)\n",
    "        preds  = (out * scale + offset).cpu().numpy()\n",
    "        id2name = {}\n",
    "        fixed_ids = []\n",
    "        for rec in records:\n",
    "            d = rec['driver']\n",
    "            id2name[d['id']] = d.get('name', str(d['id']))\n",
    "            for s in rec['sinks']:\n",
    "                id2name[s['id']] = s.get('name', str(s['id']))\n",
    "                \n",
    "        node_ids   = data.node_ids.tolist() if torch.is_tensor(data.node_ids) else data.node_ids\n",
    "        names = [id2name.get(nid, str(nid)) for nid in node_ids]\n",
    "        if hasattr(data, 'fixed_ids') and data.fixed_ids:\n",
    "            mask = ~np.isin(node_ids, data.fixed_ids)\n",
    "            names = [name for name, m in zip(names, mask) if m]\n",
    "            preds = preds[mask]\n",
    "\n",
    "        fname = f\"{data.design_name}_predictions.csv\"\n",
    "        with open(fname, \"w\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"InstanceName\", \"x_center\", \"y_center\"])\n",
    "            for name, (xv, yv) in zip(names, preds):\n",
    "                w.writerow([name, f\"{xv:.4f}\", f\"{yv:.4f}\"])\n",
    "        print(f\"Saved {fname}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45986f-fe5e-45c2-b3b6-9b9385c79bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
